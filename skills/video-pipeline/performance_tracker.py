"""YouTube Performance Tracker — daily data collection for uploaded videos.

Runs daily via cron. Pulls performance metrics for all uploaded videos
from YouTube Data API v3 and YouTube Analytics API, writes them back
to Airtable Idea Concepts table.

No analysis, no scoring — just data collection. Analysis comes in Phase 2
once there's enough data.

Prerequisites:
    - YouTube Data API v3 AND YouTube Analytics API enabled in Google Cloud
    - OAuth token with scopes: youtube.readonly, yt-analytics.readonly
      (generated by youtube_auth.py)
    - Performance fields created in Airtable (run setup_performance_fields.py)

Usage:
    python performance_tracker.py             # Update all uploaded videos
    python performance_tracker.py --recent    # Only videos from last 30 days
    python performance_tracker.py --dry-run   # Show what would be updated, don't write

Cron (VPS):
    Installed by setup_cron.sh — 7:00 AM PT daily (CRON_TZ=America/Los_Angeles)
    See setup_cron.sh for the full cron entry.
"""

import json
import sys
from datetime import datetime, timedelta, timezone
from pathlib import Path

from dotenv import load_dotenv

# Load env from project root
env_path = Path(__file__).parent.parent.parent / ".env"
load_dotenv(env_path)

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build

# Token file path (same as youtube_auth.py / youtube_uploader.py)
TOKEN_FILE = Path(__file__).parent / ".youtube-token.json"

# Scopes needed for reading stats + analytics
READ_SCOPES = [
    "https://www.googleapis.com/auth/youtube.readonly",
    "https://www.googleapis.com/auth/yt-analytics.readonly",
]

# Snapshot fields — written once, never overwritten
SNAPSHOT_FIELDS = {
    "Views 24h", "Views 48h", "Views 7d", "Views 30d",
    "CTR 48h (%)", "Retention 48h (%)",
}


def load_youtube_credentials() -> Credentials:
    """Load and refresh YouTube OAuth2 credentials from token file."""
    if not TOKEN_FILE.exists():
        raise FileNotFoundError(
            f"YouTube token not found at {TOKEN_FILE}. "
            f"Run 'python youtube_auth.py' to complete OAuth flow first."
        )

    with open(TOKEN_FILE) as f:
        token_data = json.load(f)

    creds = Credentials(
        token=token_data.get("token"),
        refresh_token=token_data.get("refresh_token"),
        token_uri=token_data.get("token_uri", "https://oauth2.googleapis.com/token"),
        client_id=token_data.get("client_id"),
        client_secret=token_data.get("client_secret"),
        scopes=READ_SCOPES,
    )

    if creds.expired and creds.refresh_token:
        creds.refresh(Request())
        # Save refreshed token
        token_data["token"] = creds.token
        with open(TOKEN_FILE, "w") as f:
            json.dump(token_data, f)

    return creds


def get_uploaded_videos(airtable_client, recent_only: bool = False) -> list[dict]:
    """Get all Airtable records that have a YouTube Video ID.

    Args:
        airtable_client: AirtableClient instance
        recent_only: If True, only return videos uploaded in the last 30 days

    Returns:
        List of Airtable record dicts with 'id' and field data
    """
    all_ideas = airtable_client.get_all_ideas()
    uploaded = [r for r in all_ideas if r.get("YouTube Video ID")]

    if recent_only:
        cutoff = datetime.now(timezone.utc) - timedelta(days=30)
        filtered = []
        for r in uploaded:
            upload_date_str = r.get("Upload Date") or r.get("Last Analytics Sync")
            if not upload_date_str:
                # No date info — include it (might be new)
                filtered.append(r)
                continue
            try:
                upload_dt = datetime.fromisoformat(upload_date_str.replace("Z", "+00:00"))
                if upload_dt >= cutoff:
                    filtered.append(r)
            except (ValueError, TypeError):
                filtered.append(r)
        uploaded = filtered

    return uploaded


def fetch_video_stats(youtube_service, video_id: str) -> dict | None:
    """Fetch lifetime stats from YouTube Data API v3.

    Uses 1 quota unit per call (videos.list with part=statistics).

    Returns:
        dict with views, likes, comments or None on failure
    """
    try:
        response = youtube_service.videos().list(
            part="statistics",
            id=video_id,
        ).execute()

        items = response.get("items", [])
        if not items:
            return None

        stats = items[0]["statistics"]
        return {
            "views": int(stats.get("viewCount", 0)),
            "likes": int(stats.get("likeCount", 0)),
            "comments": int(stats.get("commentCount", 0)),
        }
    except Exception as e:
        print(f"     Stats fetch failed: {e}")
        return None


def fetch_video_analytics(
    yt_analytics_service,
    video_id: str,
    upload_date: str | None,
) -> dict | None:
    """Fetch analytics from YouTube Analytics API.

    YouTube Analytics data has a 24-48 hour delay. This is expected.

    Returns:
        dict with impressions, ctr, avg_view_duration, avg_retention,
        watch_time_hours, subscribers_gained or None on failure
    """
    try:
        start_date = upload_date[:10] if upload_date else "2024-01-01"
        end_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")

        response = yt_analytics_service.reports().query(
            ids="channel==MINE",
            startDate=start_date,
            endDate=end_date,
            metrics=(
                "views,estimatedMinutesWatched,averageViewDuration,"
                "averageViewPercentage,subscribersGained,"
                "impressions,impressionClickThroughRate"
            ),
            filters=f"video=={video_id}",
        ).execute()

        rows = response.get("rows", [])
        if not rows or not rows[0]:
            return None

        row = rows[0]
        # Columns match the metrics order in the query
        return {
            "views": int(row[0]),
            "watch_time_hours": round(row[1] / 60, 2),
            "avg_view_duration": round(row[2], 1),
            "avg_retention": round(row[3], 1),
            "subscribers_gained": int(row[4]),
            "impressions": int(row[5]),
            "ctr": round(row[6] * 100, 2),  # API returns decimal (0.05 = 5%)
        }
    except Exception as e:
        print(f"     Analytics fetch failed: {e}")
        return None


def calculate_snapshots(
    stats: dict | None,
    analytics: dict | None,
    upload_date: str | None,
    existing_fields: dict,
) -> dict:
    """Calculate time-bucketed snapshots that haven't been written yet.

    Each snapshot is written exactly once — when the time milestone is
    first reached. This preserves the actual metric value at that point
    in time rather than overwriting with later values.

    Args:
        stats: Current lifetime stats from YouTube Data API
        analytics: Current analytics from YouTube Analytics API
        upload_date: ISO datetime string when video was published
        existing_fields: Current Airtable field values for this record

    Returns:
        dict of snapshot fields to write (only unset milestones)
    """
    if not upload_date or not stats:
        return {}

    try:
        published = datetime.fromisoformat(upload_date.replace("Z", "+00:00"))
    except (ValueError, TypeError):
        return {}

    now = datetime.now(timezone.utc)
    hours_since = (now - published).total_seconds() / 3600

    snapshots = {}

    # Only write snapshots that haven't been written yet
    if hours_since >= 24 and existing_fields.get("Views 24h") is None:
        snapshots["Views 24h"] = stats["views"]

    if hours_since >= 48:
        if existing_fields.get("Views 48h") is None:
            snapshots["Views 48h"] = stats["views"]
        if analytics:
            if existing_fields.get("CTR 48h (%)") is None:
                snapshots["CTR 48h (%)"] = analytics["ctr"]
            if existing_fields.get("Retention 48h (%)") is None:
                snapshots["Retention 48h (%)"] = analytics["avg_retention"]

    if hours_since >= 168 and existing_fields.get("Views 7d") is None:
        snapshots["Views 7d"] = stats["views"]

    if hours_since >= 720 and existing_fields.get("Views 30d") is None:
        snapshots["Views 30d"] = stats["views"]

    return snapshots


def build_slack_summary(
    results: list[dict],
    total_videos: int,
) -> str:
    """Build a Slack notification summarizing the daily sync.

    Args:
        results: List of per-video result dicts
        total_videos: Total videos processed

    Returns:
        Formatted Slack message string
    """
    successful = [r for r in results if r.get("success")]
    failed = [r for r in results if not r.get("success")]

    lines = ["*Daily Performance Update*"]
    lines.append(f"Tracked {len(successful)}/{total_videos} videos")

    if not successful:
        lines.append("No data available yet (analytics have 24-48h delay).")
        return "\n".join(lines)

    # Top performer by views
    by_views = sorted(successful, key=lambda r: r.get("views", 0), reverse=True)
    if by_views:
        top = by_views[0]
        lines.append(
            f"\nTop performer: *{top['title']}*"
            f" — {top.get('views', 0):,} views"
        )
        if top.get("ctr") is not None:
            lines.append(f"  CTR: {top['ctr']}% | Retention: {top.get('avg_retention', '?')}%")

    # Videos with low CTR (< 3%) — needs attention
    low_ctr = [r for r in successful if r.get("ctr") is not None and r["ctr"] < 3.0]
    if low_ctr:
        lines.append("\nNeeds attention (CTR < 3%):")
        for r in low_ctr:
            lines.append(f"  - {r['title']}: {r['ctr']}% CTR")

    if failed:
        lines.append(f"\n{len(failed)} videos failed to sync.")

    return "\n".join(lines)


def get_formula_performance(airtable_client) -> list[dict]:
    """Get average CTR grouped by Title Formula.

    Queries all ideas that have both a Title Formula and YouTube analytics,
    groups by formula_id, and returns average CTR per formula sorted best-first.

    Args:
        airtable_client: AirtableClient instance

    Returns:
        List of dicts: [{"formula_id": "EFF-2", "avg_ctr": 5.1, "count": 3, "titles": [...]}]
    """
    all_ideas = airtable_client.get_all_ideas()

    # Filter to records with both Title Formula and CTR data
    formula_data: dict[str, list] = {}
    for idea in all_ideas:
        formula_id = idea.get("Title Formula", "")
        ctr = idea.get("CTR (%)")
        if not formula_id or ctr is None:
            continue
        if formula_id not in formula_data:
            formula_data[formula_id] = []
        formula_data[formula_id].append({
            "title": idea.get("Video Title", "Unknown"),
            "ctr": float(ctr),
            "views": idea.get("Views", 0),
        })

    # Calculate averages and sort by CTR descending
    results = []
    for formula_id, entries in formula_data.items():
        avg_ctr = sum(e["ctr"] for e in entries) / len(entries)
        results.append({
            "formula_id": formula_id,
            "avg_ctr": round(avg_ctr, 2),
            "count": len(entries),
            "titles": [e["title"] for e in entries],
        })

    results.sort(key=lambda r: r["avg_ctr"], reverse=True)
    return results


def build_formula_report(rankings: list[dict]) -> str:
    """Build a Slack message summarizing formula performance.

    Args:
        rankings: Output from get_formula_performance()

    Returns:
        Formatted Slack message string
    """
    if not rankings:
        return "*Weekly Formula Report*\nNo videos with Title Formula + CTR data yet."

    lines = ["*Weekly Title Formula Performance*\n"]
    for i, r in enumerate(rankings, 1):
        lines.append(
            f"{i}. *{r['formula_id']}* — Avg CTR: {r['avg_ctr']}% "
            f"({r['count']} video{'s' if r['count'] != 1 else ''})"
        )
        for title in r["titles"]:
            lines.append(f"    • {title}")

    return "\n".join(lines)


def main(recent_only: bool = False, dry_run: bool = False):
    """Pull YouTube metrics and write to Airtable.

    Args:
        recent_only: Only process videos from last 30 days
        dry_run: Print what would be updated without writing
    """
    print(f"\n{'=' * 60}")
    print("YOUTUBE PERFORMANCE TRACKER")
    print(f"{'=' * 60}")
    print(f"Mode: {'DRY RUN' if dry_run else 'LIVE'}")
    print(f"Filter: {'Last 30 days' if recent_only else 'All uploaded videos'}")
    print(f"Time: {datetime.now(timezone.utc).isoformat()}")
    print(f"{'=' * 60}\n")

    # Initialize YouTube API clients
    try:
        creds = load_youtube_credentials()
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        print("Run 'python youtube_auth.py' to set up OAuth credentials.")
        sys.exit(1)

    youtube = build("youtube", "v3", credentials=creds)
    yt_analytics = build("youtubeAnalytics", "v2", credentials=creds)

    # Initialize Airtable client
    sys.path.insert(0, str(Path(__file__).parent))
    from clients.airtable_client import AirtableClient
    airtable = AirtableClient()

    # Get uploaded videos
    videos = get_uploaded_videos(airtable, recent_only=recent_only)
    if not videos:
        print("No uploaded videos found in Airtable.")
        return

    print(f"Found {len(videos)} uploaded videos\n")

    results = []

    for video in videos:
        video_id = video.get("YouTube Video ID")
        record_id = video["id"]
        title = video.get("Video Title", "Unknown")
        upload_date = video.get("Upload Date")
        formula_id = video.get("Title Formula", "")

        print(f"  {title}")
        print(f"    YouTube ID: {video_id}")
        if formula_id:
            print(f"    Title Formula: {formula_id}")

        # Fetch lifetime stats (YouTube Data API v3)
        stats = fetch_video_stats(youtube, video_id)
        if stats:
            print(f"    Views: {stats['views']:,} | Likes: {stats['likes']:,} | Comments: {stats['comments']:,}")
        else:
            print("    No stats available")

        # Fetch analytics (YouTube Analytics API)
        analytics = fetch_video_analytics(yt_analytics, video_id, upload_date)
        if analytics:
            print(f"    CTR: {analytics['ctr']}% | Retention: {analytics['avg_retention']}% | Impressions: {analytics['impressions']:,}")
        else:
            print("    No analytics available (24-48h delay is normal)")

        # Calculate time-bucketed snapshots (only write once per milestone)
        snapshots = calculate_snapshots(stats, analytics, upload_date, video)

        # Build update fields
        update_fields = {}

        if stats:
            update_fields.update({
                "Views": stats["views"],
                "Likes": stats["likes"],
                "Comments": stats["comments"],
            })

        if analytics:
            update_fields.update({
                "Impressions": analytics["impressions"],
                "CTR (%)": analytics["ctr"],
                "Avg View Duration (s)": analytics["avg_view_duration"],
                "Avg Retention (%)": analytics["avg_retention"],
                "Watch Time (hours)": analytics["watch_time_hours"],
                "Subscribers Gained": analytics["subscribers_gained"],
            })

        if snapshots:
            update_fields.update(snapshots)
            for field_name in snapshots:
                print(f"    Snapshot: {field_name} = {snapshots[field_name]}")

        update_fields["Last Analytics Sync"] = datetime.now(timezone.utc).isoformat()

        # Write to Airtable
        if dry_run:
            print(f"    [DRY RUN] Would write {len(update_fields)} fields")
        else:
            try:
                airtable.update_idea_fields(record_id, update_fields)
                print(f"    Written to Airtable ({len(update_fields)} fields)")
            except Exception as e:
                print(f"    Airtable write failed: {e}")

        # Log formula-tagged summary line
        if formula_id and (stats or analytics):
            ctr_str = f"{analytics['ctr']}%" if analytics else "N/A"
            views_str = f"{stats['views']:,}" if stats else "N/A"
            print(f"    Video: {title} | Formula: {formula_id} | 48h CTR: {ctr_str} | 48h Views: {views_str}")

        # Track result for summary
        result = {
            "title": title,
            "video_id": video_id,
            "success": stats is not None,
            "views": stats["views"] if stats else 0,
            "formula_id": formula_id,
        }
        if analytics:
            result["ctr"] = analytics["ctr"]
            result["avg_retention"] = analytics["avg_retention"]
        results.append(result)

        print()

    # Send Slack summary (non-blocking)
    if not dry_run:
        try:
            from clients.slack_client import SlackClient
            slack = SlackClient()
            summary = build_slack_summary(results, len(videos))
            slack.send_message(summary)
            print("Slack summary sent")

            # Weekly formula performance report (Sundays only)
            if datetime.now(timezone.utc).weekday() == 6:
                rankings = get_formula_performance(airtable)
                if rankings:
                    report = build_formula_report(rankings)
                    slack.send_message(report)
                    print("Weekly formula report sent")
                    for r in rankings:
                        print(f"  {r['formula_id']}: avg CTR {r['avg_ctr']}% ({r['count']} videos)")
        except Exception as e:
            print(f"Slack notification failed (non-blocking): {e}")

    print(f"{'=' * 60}")
    print(f"Done. Processed {len(videos)} videos.")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    recent = "--recent" in sys.argv
    dry = "--dry-run" in sys.argv
    main(recent_only=recent, dry_run=dry)
