"""YouTube Performance Tracker — daily data collection for uploaded videos.

Runs daily via cron. Pulls performance metrics for all uploaded videos
from YouTube Data API v3, YouTube Analytics API, and YouTube Reporting API
(bulk CSV exports for impressions/CTR), then writes them to Airtable.

No analysis, no scoring — just data collection. Analysis comes in Phase 2
once there's enough data.

Prerequisites:
    - YouTube Data API v3, YouTube Analytics API, AND YouTube Reporting API
      enabled in Google Cloud
    - OAuth token with scopes: youtube.readonly, yt-analytics.readonly
      (generated by youtube_auth.py)
    - Performance fields created in Airtable (run setup_performance_fields.py)
    - Reporting job created once: python performance_tracker.py --setup-reporting

Usage:
    python performance_tracker.py                   # Update all uploaded videos
    python performance_tracker.py --recent          # Only videos from last 30 days
    python performance_tracker.py --dry-run         # Show what would be updated
    python performance_tracker.py --setup-reporting # One-time: create bulk report job

Cron (VPS):
    Installed by setup_cron.sh — 7:00 AM PT daily (CRON_TZ=America/Los_Angeles)
    See setup_cron.sh for the full cron entry.
"""

import csv
import io
import json
import sys
from datetime import datetime, timedelta, timezone
from pathlib import Path

from dotenv import load_dotenv

# Load env from project root
env_path = Path(__file__).parent.parent.parent / ".env"
load_dotenv(env_path)

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build

# Token file path (same as youtube_auth.py / youtube_uploader.py)
TOKEN_FILE = Path(__file__).parent / ".youtube-token.json"

# Config file for persisting the Reporting API job ID (created once via --setup-reporting)
REPORTING_JOB_FILE = Path(__file__).parent / ".reporting-job.json"

# Scopes needed for reading stats + analytics + reporting
READ_SCOPES = [
    "https://www.googleapis.com/auth/youtube.readonly",
    "https://www.googleapis.com/auth/yt-analytics.readonly",
]

# Snapshot fields — written once, never overwritten
SNAPSHOT_FIELDS = {
    "Views 24h", "Views 48h", "Views 7d", "Views 30d",
    "CTR 48h (%)", "Retention 48h (%)",
}


def load_youtube_credentials() -> Credentials:
    """Load and refresh YouTube OAuth2 credentials from token file."""
    if not TOKEN_FILE.exists():
        raise FileNotFoundError(
            f"YouTube token not found at {TOKEN_FILE}. "
            f"Run 'python youtube_auth.py' to complete OAuth flow first."
        )

    with open(TOKEN_FILE) as f:
        token_data = json.load(f)

    creds = Credentials(
        token=token_data.get("token"),
        refresh_token=token_data.get("refresh_token"),
        token_uri=token_data.get("token_uri", "https://oauth2.googleapis.com/token"),
        client_id=token_data.get("client_id"),
        client_secret=token_data.get("client_secret"),
        scopes=READ_SCOPES,
    )

    if creds.expired and creds.refresh_token:
        creds.refresh(Request())
        # Save refreshed token
        token_data["token"] = creds.token
        with open(TOKEN_FILE, "w") as f:
            json.dump(token_data, f)

    return creds


def get_uploaded_videos(airtable_client, recent_only: bool = False) -> list[dict]:
    """Get all Airtable records that have a YouTube Video ID.

    Args:
        airtable_client: AirtableClient instance
        recent_only: If True, only return videos uploaded in the last 30 days

    Returns:
        List of Airtable record dicts with 'id' and field data
    """
    all_ideas = airtable_client.get_all_ideas()
    uploaded = [r for r in all_ideas if r.get("YouTube Video ID")]

    if recent_only:
        cutoff = datetime.now(timezone.utc) - timedelta(days=30)
        filtered = []
        for r in uploaded:
            upload_date_str = r.get("Upload Date") or r.get("Last Analytics Sync")
            if not upload_date_str:
                # No date info — include it (might be new)
                filtered.append(r)
                continue
            try:
                upload_dt = datetime.fromisoformat(upload_date_str.replace("Z", "+00:00"))
                if upload_dt >= cutoff:
                    filtered.append(r)
            except (ValueError, TypeError):
                filtered.append(r)
        uploaded = filtered

    return uploaded


def fetch_video_stats(youtube_service, video_id: str) -> dict | None:
    """Fetch lifetime stats from YouTube Data API v3.

    Uses 1 quota unit per call (videos.list with part=statistics).

    Returns:
        dict with views, likes, comments or None on failure
    """
    try:
        response = youtube_service.videos().list(
            part="statistics",
            id=video_id,
        ).execute()

        items = response.get("items", [])
        if not items:
            return None

        stats = items[0]["statistics"]
        return {
            "views": int(stats.get("viewCount", 0)),
            "likes": int(stats.get("likeCount", 0)),
            "comments": int(stats.get("commentCount", 0)),
        }
    except Exception as e:
        print(f"     Stats fetch failed: {e}")
        return None


def fetch_video_analytics(
    yt_analytics_service,
    video_id: str,
    upload_date: str | None,
) -> dict | None:
    """Fetch analytics from YouTube Analytics API.

    YouTube Analytics data has a 24-48 hour delay. This is expected.

    Note: impressions and impressionClickThroughRate are NOT available
    in the Analytics API v2 reports.query endpoint for per-video queries.
    Those require the bulk Reporting API (channel_reach_basic_a1 reports).
    We fetch what's available: watch time, avg duration, retention, subs.

    Returns:
        dict with avg_view_duration, avg_retention, watch_time_hours,
        subscribers_gained or None on failure
    """
    try:
        start_date = upload_date[:10] if upload_date else "2024-01-01"
        end_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")

        response = yt_analytics_service.reports().query(
            ids="channel==MINE",
            startDate=start_date,
            endDate=end_date,
            metrics=(
                "views,estimatedMinutesWatched,averageViewDuration,"
                "averageViewPercentage,subscribersGained"
            ),
            filters=f"video=={video_id}",
        ).execute()

        rows = response.get("rows", [])
        if not rows or not rows[0]:
            return None

        row = rows[0]
        # Columns match the metrics order in the query
        return {
            "views": int(row[0]),
            "watch_time_hours": round(row[1] / 60, 2),
            "avg_view_duration": round(row[2], 1),
            "avg_retention": round(row[3], 1),
            "subscribers_gained": int(row[4]),
        }
    except Exception as e:
        print(f"     Analytics fetch failed: {e}")
        return None


def create_reporting_job(yt_reporting_service) -> str:
    """Create a YouTube Reporting API job for channel reach data.

    This only needs to run ONCE. After creation, Google generates daily
    CSV reports automatically. The job ID is saved to .reporting-job.json.

    Report type: channel_reach_basic_a1
    Includes: video_id, video_thumbnail_impressions, video_thumbnail_impressions_ctr

    Args:
        yt_reporting_service: YouTube Reporting API service instance

    Returns:
        Job ID string
    """
    # Check if job already exists
    if REPORTING_JOB_FILE.exists():
        with open(REPORTING_JOB_FILE) as f:
            data = json.load(f)
        job_id = data.get("job_id")
        if job_id:
            print(f"Reporting job already exists: {job_id}")
            print(f"Created: {data.get('created_at', 'unknown')}")
            return job_id

    # List existing jobs to avoid duplicates
    existing = yt_reporting_service.jobs().list().execute()
    for job in existing.get("jobs", []):
        if job.get("reportTypeId") == "channel_reach_basic_a1":
            job_id = job["id"]
            print(f"Found existing reporting job: {job_id}")
            # Save it
            with open(REPORTING_JOB_FILE, "w") as f:
                json.dump({
                    "job_id": job_id,
                    "report_type": "channel_reach_basic_a1",
                    "created_at": datetime.now(timezone.utc).isoformat(),
                    "note": "Recovered from existing YouTube job",
                }, f, indent=2)
            return job_id

    # Create new job
    response = yt_reporting_service.jobs().create(
        body={
            "reportTypeId": "channel_reach_basic_a1",
            "name": "Economy FastForward — Reach & CTR",
        }
    ).execute()

    job_id = response["id"]
    print(f"Created reporting job: {job_id}")
    print(f"Report type: channel_reach_basic_a1")
    print(f"Google will generate the first report within 24-48 hours.")

    # Persist job ID
    with open(REPORTING_JOB_FILE, "w") as f:
        json.dump({
            "job_id": job_id,
            "report_type": "channel_reach_basic_a1",
            "created_at": datetime.now(timezone.utc).isoformat(),
        }, f, indent=2)

    return job_id


def _download_report(yt_reporting_service, download_url: str) -> str | None:
    """Download a single report CSV from the Reporting API.

    Uses the googleapiclient media download with the downloadUrl from
    jobs.reports.list(). The resourceName=" " trick is from Google's
    official sample code — the actual URL is overridden.

    Returns:
        CSV text as string, or None on failure.
    """
    try:
        from googleapiclient.http import MediaIoBaseDownload
        buf = io.BytesIO()
        request = yt_reporting_service.media().download(resourceName=" ")
        request.uri = download_url
        downloader = MediaIoBaseDownload(buf, request, chunksize=-1)
        done = False
        while not done:
            _, done = downloader.next_chunk()
        return buf.getvalue().decode("utf-8")
    except Exception as e:
        print(f"    Report download failed: {e}")
        return None


def fetch_reporting_data(yt_reporting_service) -> dict[str, dict]:
    """Fetch per-video impressions and CTR from YouTube Reporting API bulk exports.

    Downloads ALL available CSV reports and aggregates across all dates
    to get lifetime totals per video. Reports have a 24-72 hour delay.

    Args:
        yt_reporting_service: YouTube Reporting API service instance

    Returns:
        Dict mapping video_id → {"impressions": int, "ctr": float}
        Empty dict if no reports available yet.
    """
    # Load job ID
    if not REPORTING_JOB_FILE.exists():
        print("  No reporting job configured. Run --setup-reporting first.")
        return {}

    with open(REPORTING_JOB_FILE) as f:
        job_id = json.load(f).get("job_id")

    if not job_id:
        return {}

    # List all available reports
    try:
        reports = []
        page_token = None
        while True:
            kwargs = {"jobId": job_id}
            if page_token:
                kwargs["pageToken"] = page_token
            resp = yt_reporting_service.jobs().reports().list(**kwargs).execute()
            reports.extend(resp.get("reports", []))
            page_token = resp.get("nextPageToken")
            if not page_token:
                break
    except Exception as e:
        print(f"  Reporting API list failed: {e}")
        return {}

    if not reports:
        print("  No bulk reports available yet (first report takes 24-48h after job creation).")
        return {}

    print(f"  Found {len(reports)} bulk reports, downloading...")

    # Aggregate across all reports (each covers one day)
    # Track impressions and estimated clicks to compute weighted average CTR
    video_data: dict[str, dict] = {}  # video_id → {impressions, clicks}

    for report in reports:
        download_url = report.get("downloadUrl")
        if not download_url:
            continue

        csv_text = _download_report(yt_reporting_service, download_url)
        if not csv_text:
            continue

        reader = csv.DictReader(io.StringIO(csv_text))
        for row in reader:
            vid = row.get("video_id", "").strip()
            if not vid:
                continue

            try:
                impressions = int(row.get("video_thumbnail_impressions", 0))
                ctr = float(row.get("video_thumbnail_impressions_ctr", 0))
            except (ValueError, TypeError):
                continue

            if vid not in video_data:
                video_data[vid] = {"impressions": 0, "clicks": 0}

            video_data[vid]["impressions"] += impressions
            # CTR is per-row ratio — back-calculate clicks for weighted average
            video_data[vid]["clicks"] += round(impressions * ctr)

    # Convert to final format: impressions + weighted average CTR (as percentage)
    result = {}
    for vid, data in video_data.items():
        imp = data["impressions"]
        ctr = round((data["clicks"] / imp) * 100, 2) if imp > 0 else 0.0
        result[vid] = {"impressions": imp, "ctr": ctr}

    print(f"  Parsed reporting data for {len(result)} videos")
    return result


def calculate_snapshots(
    stats: dict | None,
    analytics: dict | None,
    upload_date: str | None,
    existing_fields: dict,
    reporting: dict | None = None,
) -> dict:
    """Calculate time-bucketed snapshots that haven't been written yet.

    Each snapshot is written exactly once — when the time milestone is
    first reached. This preserves the actual metric value at that point
    in time rather than overwriting with later values.

    Args:
        stats: Current lifetime stats from YouTube Data API
        analytics: Current analytics from YouTube Analytics API
        upload_date: ISO datetime string when video was published
        existing_fields: Current Airtable field values for this record
        reporting: CTR/impressions data from Reporting API (optional)

    Returns:
        dict of snapshot fields to write (only unset milestones)
    """
    if not upload_date or not stats:
        return {}

    try:
        published = datetime.fromisoformat(upload_date.replace("Z", "+00:00"))
    except (ValueError, TypeError):
        return {}

    now = datetime.now(timezone.utc)
    hours_since = (now - published).total_seconds() / 3600

    snapshots = {}

    # Only write snapshots that haven't been written yet
    if hours_since >= 24 and existing_fields.get("Views 24h") is None:
        snapshots["Views 24h"] = stats["views"]

    if hours_since >= 48:
        if existing_fields.get("Views 48h") is None:
            snapshots["Views 48h"] = stats["views"]
        if analytics:
            if existing_fields.get("Retention 48h (%)") is None:
                snapshots["Retention 48h (%)"] = analytics["avg_retention"]
        if reporting:
            if existing_fields.get("CTR 48h (%)") is None:
                snapshots["CTR 48h (%)"] = reporting["ctr"]

    if hours_since >= 168 and existing_fields.get("Views 7d") is None:
        snapshots["Views 7d"] = stats["views"]

    if hours_since >= 720 and existing_fields.get("Views 30d") is None:
        snapshots["Views 30d"] = stats["views"]

    return snapshots


def build_slack_summary(
    results: list[dict],
    total_videos: int,
) -> str:
    """Build a Slack notification summarizing the daily sync.

    Args:
        results: List of per-video result dicts
        total_videos: Total videos processed

    Returns:
        Formatted Slack message string
    """
    successful = [r for r in results if r.get("success")]
    failed = [r for r in results if not r.get("success")]

    lines = ["*Daily Performance Update*"]
    lines.append(f"Tracked {len(successful)}/{total_videos} videos")

    if not successful:
        lines.append("No data available yet (analytics have 24-48h delay).")
        return "\n".join(lines)

    # Top performer by views
    by_views = sorted(successful, key=lambda r: r.get("views", 0), reverse=True)
    if by_views:
        top = by_views[0]
        lines.append(
            f"\nTop performer: *{top['title']}*"
            f" — {top.get('views', 0):,} views"
        )
        detail_parts = []
        if top.get("ctr") is not None:
            detail_parts.append(f"CTR: {top['ctr']}%")
        if top.get("avg_retention") is not None:
            detail_parts.append(f"Retention: {top['avg_retention']}%")
        if detail_parts:
            lines.append(f"  {' | '.join(detail_parts)}")

    # Videos with low CTR (< 3%) — needs attention
    low_ctr = [r for r in successful if r.get("ctr") is not None and r["ctr"] < 3.0]
    if low_ctr:
        lines.append("\nNeeds attention (CTR < 3%):")
        for r in low_ctr:
            lines.append(f"  - {r['title']}: {r['ctr']}% CTR")

    if failed:
        lines.append(f"\n{len(failed)} videos failed to sync.")

    return "\n".join(lines)


def get_formula_performance(airtable_client) -> list[dict]:
    """Get average CTR grouped by Title Formula.

    Queries all ideas that have both a Title Formula and CTR data,
    groups by formula_id, and returns average CTR per formula sorted best-first.
    Falls back to retention if no CTR data exists yet.

    Args:
        airtable_client: AirtableClient instance

    Returns:
        List of dicts: [{"formula_id": "EFF-2", "avg_ctr": 5.1, "count": 3, "titles": [...]}]
    """
    all_ideas = airtable_client.get_all_ideas()

    # Filter to records with both Title Formula and CTR data
    formula_data: dict[str, list] = {}
    for idea in all_ideas:
        formula_id = idea.get("Title Formula", "")
        ctr = idea.get("CTR (%)")
        if not formula_id or ctr is None:
            continue
        if formula_id not in formula_data:
            formula_data[formula_id] = []
        formula_data[formula_id].append({
            "title": idea.get("Video Title", "Unknown"),
            "ctr": float(ctr),
            "views": idea.get("Views", 0),
        })

    # Calculate averages and sort by CTR descending
    results = []
    for formula_id, entries in formula_data.items():
        avg_ctr = sum(e["ctr"] for e in entries) / len(entries)
        results.append({
            "formula_id": formula_id,
            "avg_ctr": round(avg_ctr, 2),
            "count": len(entries),
            "titles": [e["title"] for e in entries],
        })

    results.sort(key=lambda r: r["avg_ctr"], reverse=True)
    return results


def build_formula_report(rankings: list[dict]) -> str:
    """Build a Slack message summarizing formula performance.

    Args:
        rankings: Output from get_formula_performance()

    Returns:
        Formatted Slack message string
    """
    if not rankings:
        return "*Weekly Formula Report*\nNo videos with Title Formula + CTR data yet."

    lines = ["*Weekly Title Formula Performance*\n"]
    for i, r in enumerate(rankings, 1):
        lines.append(
            f"{i}. *{r['formula_id']}* — Avg CTR: {r['avg_ctr']}% "
            f"({r['count']} video{'s' if r['count'] != 1 else ''})"
        )
        for title in r["titles"]:
            lines.append(f"    • {title}")

    return "\n".join(lines)


def main(recent_only: bool = False, dry_run: bool = False):
    """Pull YouTube metrics and write to Airtable.

    Args:
        recent_only: Only process videos from last 30 days
        dry_run: Print what would be updated without writing
    """
    print(f"\n{'=' * 60}")
    print("YOUTUBE PERFORMANCE TRACKER")
    print(f"{'=' * 60}")
    print(f"Mode: {'DRY RUN' if dry_run else 'LIVE'}")
    print(f"Filter: {'Last 30 days' if recent_only else 'All uploaded videos'}")
    print(f"Time: {datetime.now(timezone.utc).isoformat()}")
    print(f"{'=' * 60}\n")

    # Initialize YouTube API clients
    try:
        creds = load_youtube_credentials()
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        print("Run 'python youtube_auth.py' to set up OAuth credentials.")
        sys.exit(1)

    youtube = build("youtube", "v3", credentials=creds)
    yt_analytics = build("youtubeAnalytics", "v2", credentials=creds)
    yt_reporting = build("youtubereporting", "v1", credentials=creds)

    # Initialize Airtable client
    sys.path.insert(0, str(Path(__file__).parent))
    from clients.airtable_client import AirtableClient
    airtable = AirtableClient()

    # Get uploaded videos
    videos = get_uploaded_videos(airtable, recent_only=recent_only)
    if not videos:
        print("No uploaded videos found in Airtable.")
        return

    print(f"Found {len(videos)} uploaded videos\n")

    # Fetch bulk reporting data (impressions + CTR) — one call for all videos
    print("Fetching bulk reporting data (impressions/CTR)...")
    reporting_data = fetch_reporting_data(yt_reporting)
    print()

    results = []

    for video in videos:
        video_id = video.get("YouTube Video ID")
        record_id = video["id"]
        title = video.get("Video Title", "Unknown")
        upload_date = video.get("Upload Date")
        formula_id = video.get("Title Formula", "")

        print(f"  {title}")
        print(f"    YouTube ID: {video_id}")
        if formula_id:
            print(f"    Title Formula: {formula_id}")

        # Fetch lifetime stats (YouTube Data API v3)
        stats = fetch_video_stats(youtube, video_id)
        if stats:
            print(f"    Views: {stats['views']:,} | Likes: {stats['likes']:,} | Comments: {stats['comments']:,}")
        else:
            print("    No stats available")

        # Fetch analytics (YouTube Analytics API)
        analytics = fetch_video_analytics(yt_analytics, video_id, upload_date)
        if analytics:
            print(f"    Retention: {analytics['avg_retention']}% | Avg Duration: {analytics['avg_view_duration']}s | Watch Time: {analytics['watch_time_hours']}h")
        else:
            print("    No analytics available (24-48h delay is normal)")

        # Match reporting data (impressions/CTR from bulk exports)
        reporting = reporting_data.get(video_id)
        if reporting:
            print(f"    Impressions: {reporting['impressions']:,} | CTR: {reporting['ctr']}%")

        # Calculate time-bucketed snapshots (only write once per milestone)
        snapshots = calculate_snapshots(stats, analytics, upload_date, video, reporting)

        # Build update fields
        update_fields = {}

        if stats:
            update_fields.update({
                "Views": stats["views"],
                "Likes": stats["likes"],
                "Comments": stats["comments"],
            })

        if analytics:
            update_fields.update({
                "Avg View Duration (s)": analytics["avg_view_duration"],
                "Avg Retention (%)": analytics["avg_retention"],
                "Watch Time (hours)": analytics["watch_time_hours"],
                "Subscribers Gained": analytics["subscribers_gained"],
            })

        if reporting:
            update_fields.update({
                "Impressions": reporting["impressions"],
                "CTR (%)": reporting["ctr"],
            })

        if snapshots:
            update_fields.update(snapshots)
            for field_name in snapshots:
                print(f"    Snapshot: {field_name} = {snapshots[field_name]}")

        update_fields["Last Analytics Sync"] = datetime.now(timezone.utc).isoformat()

        # Write to Airtable
        if dry_run:
            print(f"    [DRY RUN] Would write {len(update_fields)} fields")
        else:
            try:
                airtable.update_idea_fields(record_id, update_fields)
                print(f"    Written to Airtable ({len(update_fields)} fields)")
            except Exception as e:
                print(f"    Airtable write failed: {e}")

        # Log formula-tagged summary line
        if formula_id and (stats or analytics or reporting):
            ctr_str = f"{reporting['ctr']}%" if reporting else "N/A"
            views_str = f"{stats['views']:,}" if stats else "N/A"
            print(f"    Formula: {formula_id} | CTR: {ctr_str} | Views: {views_str}")

        # Track result for summary
        result = {
            "title": title,
            "video_id": video_id,
            "success": stats is not None,
            "views": stats["views"] if stats else 0,
            "formula_id": formula_id,
        }
        if analytics:
            result["avg_retention"] = analytics["avg_retention"]
        if reporting:
            result["ctr"] = reporting["ctr"]
        results.append(result)

        print()

    # Send Slack summary (non-blocking)
    if not dry_run:
        try:
            from clients.slack_client import SlackClient
            slack = SlackClient()
            summary = build_slack_summary(results, len(videos))
            slack.send_message(summary)
            print("Slack summary sent")

            # Weekly formula performance report (Sundays only)
            if datetime.now(timezone.utc).weekday() == 6:
                rankings = get_formula_performance(airtable)
                if rankings:
                    report = build_formula_report(rankings)
                    slack.send_message(report)
                    print("Weekly formula report sent")
                    for r in rankings:
                        print(f"  {r['formula_id']}: avg CTR {r['avg_ctr']}% ({r['count']} videos)")
        except Exception as e:
            print(f"Slack notification failed (non-blocking): {e}")

    print(f"{'=' * 60}")
    print(f"Done. Processed {len(videos)} videos.")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    if "--setup-reporting" in sys.argv:
        print("Setting up YouTube Reporting API job...")
        _creds = load_youtube_credentials()
        _yt_reporting = build("youtubereporting", "v1", credentials=_creds)
        job_id = create_reporting_job(_yt_reporting)
        print(f"\nJob ID: {job_id}")
        print("Google will generate the first report within 24-48 hours.")
        print("After that, daily reports are generated automatically.")
    else:
        recent = "--recent" in sys.argv
        dry = "--dry-run" in sys.argv
        main(recent_only=recent, dry_run=dry)
